{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPOOoA27BA_w",
        "outputId": "f7e34597-c8d0-4507-9b75-44a8d9260a63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/111.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/137.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/500.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m500.5/500.5 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/158.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.1/158.1 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hEnter your Groq API Key: ··········\n"
          ]
        }
      ],
      "source": [
        "# Setup\n",
        "%pip install python-dotenv --upgrade --quiet langchain langchain-groq\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "if \"GROQ_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API Key: \")\n",
        "\n",
        "# Using Llama3.1-8b (Small/Fast) to demonstrate logic failures\n",
        "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many does he have now?\"\n",
        "\n",
        "# 1. Standard Prompt (Direct Answer)\n",
        "prompt_standard = f\"Answer this question: {question}\"\n",
        "print(\"--- STANDARD (Llama3.1-8b) ---\")\n",
        "print(llm.invoke(prompt_standard).content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UACjXtfLBG_I",
        "outputId": "be209633-4c27-47b0-ef56-1668652eb865"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- STANDARD (Llama3.1-8b) ---\n",
            "To find out how many tennis balls Roger has now, we need to add the initial number of tennis balls he had (5) to the number of tennis balls he bought (2 cans * 3 tennis balls per can).\n",
            "\n",
            "2 cans * 3 tennis balls per can = 6 tennis balls\n",
            "\n",
            "Now, let's add the initial number of tennis balls (5) to the number of tennis balls he bought (6):\n",
            "\n",
            "5 + 6 = 11\n",
            "\n",
            "So, Roger now has 11 tennis balls.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. CoT Prompt (Magic Phrase)\n",
        "prompt_cot = f\"Answer this question. Let's think step by step. {question}\"\n",
        "\n",
        "print(\"--- Chain of Thought (Llama3.1-8b) ---\")\n",
        "print(llm.invoke(prompt_cot).content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiqqulyyBPPj",
        "outputId": "f500746f-86cd-4171-cc22-14f2a783de60"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Chain of Thought (Llama3.1-8b) ---\n",
            "To find out how many tennis balls Roger has now, we need to follow these steps:\n",
            "\n",
            "1. Roger already has 5 tennis balls.\n",
            "2. He buys 2 more cans of tennis balls. Each can has 3 tennis balls, so he buys 2 x 3 = 6 more tennis balls.\n",
            "3. Now, we add the tennis balls he already had (5) to the new tennis balls he bought (6). 5 + 6 = 11\n",
            "\n",
            "So, Roger now has 11 tennis balls.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "%pip install python-dotenv --upgrade --quiet langchain langchain-groq\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "if \"GROQ_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API Key: \")\n",
        "\n",
        "# Using Llama3.1-8b\n",
        "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.7) # Creativity needed\n"
      ],
      "metadata": {
        "id": "hkb2pcXbBRp9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "problem = \"How can I get my 5-year-old to eat vegetables?\"\n",
        "\n",
        "# Step 1: The Branch Generator\n",
        "prompt_branch = ChatPromptTemplate.from_template(\n",
        "    \"Problem: {problem}. Give me one unique, creative solution. Solution {id}:\"\n",
        ")\n",
        "\n",
        "branches = RunnableParallel(\n",
        "    sol1=prompt_branch.partial(id=\"1\") | llm | StrOutputParser(),\n",
        "    sol2=prompt_branch.partial(id=\"2\") | llm | StrOutputParser(),\n",
        "    sol3=prompt_branch.partial(id=\"3\") | llm | StrOutputParser(),\n",
        ")\n",
        "\n",
        "# Step 2: The Judge\n",
        "prompt_judge = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    I have three proposed solutions for: '{problem}'\n",
        "\n",
        "    1: {sol1}\n",
        "    2: {sol2}\n",
        "    3: {sol3}\n",
        "\n",
        "    Act as a Child Psychologist. Pick the most sustainable one (not bribery) and explain why.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# Chain: Input -> Branches -> Judge -> Output\n",
        "tot_chain = (\n",
        "    RunnableParallel(problem=RunnableLambda(lambda x: x), branches=branches)\n",
        "    | (lambda x: {**x[\"branches\"], \"problem\": x[\"problem\"]})\n",
        "    | prompt_judge\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"--- Tree of Thoughts (ToT) Result ---\")\n",
        "print(tot_chain.invoke(problem))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21qsIiDhBU_P",
        "outputId": "3b9a5faf-b4e3-4012-9fdc-51905da69e09"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tree of Thoughts (ToT) Result ---\n",
            "As a Child Psychologist, I would recommend **Solution 3: The \"Rainbow Plate\" Challenge** as the most sustainable approach to encourage a 5-year-old to eat vegetables. Here's why:\n",
            "\n",
            "1. **Involves child in the process**: This solution encourages the child to participate in the preparation and arrangement of the vegetables. This involvement fosters a sense of ownership and responsibility, making the child more invested in trying the vegetables.\n",
            "2. **Promotes learning and exploration**: By creating a \"rainbow garden\" on the plate, the child is encouraged to learn about colors, textures, and shapes, promoting cognitive development and exploration.\n",
            "3. **Fosters creativity and imagination**: The challenge to come up with a story about the \"rainbow garden\" allows the child to express their creativity and imagination, building confidence and self-esteem.\n",
            "4. **Does not rely on bribery**: Unlike the \"Veggie Treasure Hunt\" and \"Veggie Face\" challenges, which use rewards and stickers, the \"Rainbow Plate\" Challenge focuses on the process of creating a fun and engaging meal experience, rather than relying on external motivators.\n",
            "5. **Encourages self-regulation**: By involving the child in the preparation and arrangement of the vegetables, this solution encourages self-regulation, as the child learns to manage their own eating habits and preferences.\n",
            "6. **Supports healthy eating habits**: This approach promotes healthy eating habits by making mealtime a positive and enjoyable experience, which can lead to a lifelong appreciation for vegetables and other nutritious foods.\n",
            "\n",
            "Overall, the \"Rainbow Plate\" Challenge is a sustainable and effective approach to encouraging a 5-year-old to eat vegetables, as it promotes learning, creativity, and self-regulation while avoiding the use of bribery.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. The Generator (Divergence)\n",
        "prompt_draft = ChatPromptTemplate.from_template(\n",
        "    \"Write a 1-sentence movie plot about: {topic}. Genre: {genre}.\"\n",
        ")\n",
        "\n",
        "drafts = RunnableParallel(\n",
        "    draft_scifi=prompt_draft.partial(genre=\"Sci-Fi\") | llm | StrOutputParser(),\n",
        "    draft_romance=prompt_draft.partial(genre=\"Romance\") | llm | StrOutputParser(),\n",
        "    draft_horror=prompt_draft.partial(genre=\"Horror\") | llm | StrOutputParser(),\n",
        ")\n",
        "\n",
        "# 2. The Aggregator (Convergence)\n",
        "prompt_combine = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    I have three movie ideas for the topic '{topic}':\n",
        "    1. Sci-Fi: {draft_scifi}\n",
        "    2. Romance: {draft_romance}\n",
        "    3. Horror: {draft_horror}\n",
        "\n",
        "    Your task: Create a new Mega-Movie that combines the TECHNOLOGY of Sci-Fi, the PASSION of Romance, and the FEAR of Horror.\n",
        "    Write one paragraph.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# 3. The Chain\n",
        "got_chain = (\n",
        "    RunnableParallel(topic=RunnableLambda(lambda x: x), drafts=drafts)\n",
        "    | (lambda x: {**x[\"drafts\"], \"topic\": x[\"topic\"]})\n",
        "    | prompt_combine\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"--- Graph of Thoughts (GoT) Result ---\")\n",
        "print(got_chain.invoke(\"Time Travel\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03gA4WDABXO_",
        "outputId": "ea977770-78ad-4dd6-88a9-338b8ce66a2a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Graph of Thoughts (GoT) Result ---\n",
            "In \"Echoes of Eternity,\" a brilliant but reclusive physicist named Maya discovers a way to manipulate the space-time continuum, allowing her to travel through the ages. As she becomes increasingly obsessed with her invention, she meets a charismatic stranger named Asher, who claims to have been sent from a future where their love has transcended time itself. But when Maya uses her technology to alter the course of events, she finds herself trapped in a nightmarish loop of reliving the same fateful evening over and over, stalked by a vengeful killer from her own dark past who has been awakened by her tampering with the timeline. As Maya navigates the blurred lines between past, present, and future, she must confront the consequences of her actions and the true nature of her love for Asher, all while trying to survive the relentless pursuit of her stalker and prevent a catastrophic future from unfolding.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Df40txPqBZn2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}